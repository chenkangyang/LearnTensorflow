{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "tf.set_random_seed(1)\n",
    "lr = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "\n",
    "n_inputs = 28   # MNIST data input (img shape: 28*28) 每次读入一行1*28\n",
    "n_steps = 28    # time steps 纵向移动28次\n",
    "n_hidden_units = 128   # neurons in hidden layer\n",
    "n_classes = 10      # MNIST classes (0-9 digits)\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights\n",
    "weights = {\n",
    "    # (28, 128)\n",
    "    'in': tf.Variable(tf.random_normal([n_inputs, n_hidden_units])),\n",
    "    # (128, 10)\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_units, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    # (128, )\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ])),\n",
    "    # (10, )\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造网络\n",
    "\n",
    "不考虑batch_size时\n",
    "```\n",
    "X = (28,28)\n",
    "\n",
    "对于X的每一行：\n",
    "\n",
    "for X_line in X:\n",
    "    1.经隐藏层w=(28,128): X_line=(1,128)\n",
    "    每个step得到一个X_line(1,128)\n",
    "    2.经LSTM_Cell\n",
    "    每个step得到(1,128)的outpus\n",
    "    3.经隐藏层w=(128,10): \n",
    "    每个step得到(1,10)results\n",
    "    \n",
    "    \n",
    "    \n",
    "共进行28 steps，得到28个results(1,10)\n",
    "\n",
    "最终取，最后一个results = final_state[1]*w+b\n",
    "```\n",
    "考虑batch_size时，即增加深度。想象把128张“例图”重叠的放置在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(X, weights, biases):\n",
    "    # hidden layer for input to cell\n",
    "    ########################################\n",
    "    \n",
    "    # transpose the inputs shape from\n",
    "    # X ==> (128 batch * 28 steps, 28 inputs)\n",
    "    X = tf.reshape(X, [-1, n_inputs])\n",
    "\n",
    "    # into hidden\n",
    "    # X_in = (128 batch * 28 steps, 128 hidden)\n",
    "    X_in = tf.matmul(X, weights['in']) + biases['in']\n",
    "    # X_in ==> (128 batch, 28 steps, 128 hidden)\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_units])\n",
    "\n",
    "    \n",
    "    # cell\n",
    "    ##########################################\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units)\n",
    "    # lstm cell is divided into two parts (c_state, m_state)\n",
    "    init_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "    outpus, final_state = tf.nn.dynamic_rnn(cell, X_in, initial_state=init_state,time_major=False)\n",
    "    # 由于 steps 即时间序列长度为28，在X_in的次要维度，\n",
    "    # 所以time_major=False，否则，28 steps 在X_in的第一个维度，\n",
    "    # 则time_major=True\n",
    "\n",
    "    # hidden layer for output as the final results\n",
    "    ############################################\n",
    "    results = tf.matmul(final_state[1], weights['out']) + biases['out']\n",
    "    # or\n",
    "    # unpack to list [(batch, outputs)..] * steps\n",
    "    # transpose 三维张量转制，102代表第一第二维度交换\n",
    "    # outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))\n",
    "    # results = tf.matmul(outputs[-1], weights['out']) + biases['out'] # shape = (128, 10)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-a295e5f80f45>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = RNN(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集中前128个图的准确度:\n",
      "每次读取128张图，第 1 次\n",
      "0.1953125\n",
      "每次读取128张图，第 51 次\n",
      "0.828125\n",
      "每次读取128张图，第 101 次\n",
      "0.8359375\n",
      "每次读取128张图，第 151 次\n",
      "0.8984375\n",
      "每次读取128张图，第 201 次\n",
      "0.9296875\n",
      "每次读取128张图，第 251 次\n",
      "0.9296875\n",
      "每次读取128张图，第 301 次\n",
      "0.921875\n",
      "每次读取128张图，第 351 次\n",
      "0.953125\n",
      "每次读取128张图，第 401 次\n",
      "0.9296875\n",
      "每次读取128张图，第 451 次\n",
      "0.9609375\n",
      "每次读取128张图，第 501 次\n",
      "0.9609375\n",
      "每次读取128张图，第 551 次\n",
      "0.9375\n",
      "每次读取128张图，第 601 次\n",
      "0.953125\n",
      "每次读取128张图，第 651 次\n",
      "0.96875\n",
      "每次读取128张图，第 701 次\n",
      "0.9375\n",
      "每次读取128张图，第 751 次\n",
      "0.9921875\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    x_test = mnist.test.images[:128]\n",
    "    y_test = mnist.test.labels[:128]\n",
    "    x_test = x_test.reshape([128, n_steps, n_inputs])\n",
    "    print(\"测试集中前128个图的准确度:\")\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape([batch_size, n_steps, n_inputs])\n",
    "        sess.run([train_op], feed_dict={x: batch_xs,\n",
    "                                        y: batch_ys})\n",
    "        if step % 50 == 0:\n",
    "            print(\"每次读取128张图，第\",step+1,\"次\")\n",
    "            print(sess.run(accuracy, feed_dict={\n",
    "                x:x_test,\n",
    "                y:y_test\n",
    "        }))\n",
    "        step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gc]",
   "language": "python",
   "name": "conda-env-gc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
